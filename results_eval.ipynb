{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-15 11:35:44.584197: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Applications/anaconda3/lib/python3.11/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/Applications/anaconda3/lib/python3.11/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/Applications/anaconda3/lib/python3.11/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(filename):\n",
    "    \"\"\"\n",
    "    Read a jsonlines file into a list of dictionaries.\n",
    "\n",
    "    Args:\n",
    "        filename (str): Path to input file.\n",
    "\n",
    "    Returns:\n",
    "        ([dict]): List of dictionaries.\n",
    "    \"\"\"\n",
    "    with open(filename, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        samples = []\n",
    "        for l in lines:\n",
    "            samples.append(json.loads(l))\n",
    "        return samples\n",
    "\n",
    "\n",
    "def dump_jsonl(d, filename):\n",
    "    \"\"\"\n",
    "    Dump a list of dictionaries to file as jsonlines.\n",
    "\n",
    "    Args:\n",
    "        d ([dict]): List of dictionaries.\n",
    "        filename (str): Path to output file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        for entry in d:\n",
    "            f.write(json.dumps(entry) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result object\n",
    "class AssertInfo(object):\n",
    "    def __init__(self, idx, res_idx, content_keys):\n",
    "        self.val_index = idx\n",
    "        self.res_index = res_idx\n",
    "        self.data = []\n",
    "        self.structure_id = True\n",
    "        self.test_account = {e: set() for e in content_keys}\n",
    "        self.gold_account = {e: set() for e in content_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"/Users/yuz/Work/reprod/evaluate/metrics/rouge/rouge.py\")\n",
    "\n",
    "def get_tokens(s):\n",
    "    if isinstance(s, str):\n",
    "        stripped =  [e.strip() for e in s.split(\" \")]\n",
    "    else:\n",
    "        stripped = [s]\n",
    "    return [e for e in stripped if e]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    {\n",
    "        \"Sample\": [{\n",
    "            \"Diseases\": \"Emphysematous cystitis\", \n",
    "            \"Symptoms\": [\"Nausea\", \"Vomiting\", \"Macrohematuria\"], \n",
    "            \"TherapeuticsInformation\": [{\n",
    "                \"TherapeuticsApproach\": \"Antibiotics\", \n",
    "                \"CureRate\": \"0\", \n",
    "                \"DrugFrequency\": \"\"}, {\n",
    "                \"TherapeuticsApproach\": \"Simple cystectomy and ureterocutaneostomy\", \n",
    "                \"CureRate\": \"1\", \n",
    "                \"DrugFrequency\": \"\"\n",
    "            }], \n",
    "        \"DiagnosisMethods\": [\"Cystoscopy\", \"Histological examination\",\n",
    "                             \"Urine and mucosal surface cultures\"], \n",
    "        \"Causes\": \"Escherichia coli infection\", \n",
    "        \"MedicalHistory\": \"Insulin-dependent diabetes mellitus (IDDM)\"\n",
    "        }]\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "def diff_json(response_data, assert_data, content_keys, assert_info, content_key):\n",
    "    if isinstance(response_data, dict):\n",
    "        for key in assert_data:\n",
    "            if key not in response_data:\n",
    "                print(f'structure unidentified with id {assert_info.val_index} in dict condition [1] with content key: {content_key} and response_data: {response_data}')\n",
    "                assert_info.structure_id = False\n",
    "                return\n",
    "            \n",
    "        for key in response_data:\n",
    "            if key in assert_data:\n",
    "                diff_json(response_data[key], assert_data[key], content_keys, assert_info, key)\n",
    "            else:\n",
    "                print(f'structure unidentified with id {assert_info.val_index} in dict condition [2] with content key: {content_key} and response_data: {response_data}')\n",
    "                assert_info.structure_id = False\n",
    "                return\n",
    "    elif isinstance(response_data, list):\n",
    "        if len(response_data) != 0 and len(assert_data)!=0 and len(response_data) != len(assert_data) and isinstance(assert_data[0], dict):\n",
    "            print(f'structure unidentified with id {assert_info.val_index} in list condition [1] with content key: {content_key} and response_data: {response_data}')\n",
    "            assert_info.structure_id = False\n",
    "            return\n",
    "\n",
    "        if response_data and assert_data and not isinstance(response_data[0], type(assert_data[0])):\n",
    "            print(f'structure unidentified with id {assert_info.val_index} in list condition [2] with content key: {content_key} and response_data: {response_data}')\n",
    "            assert_info.structure_id = False\n",
    "            return \n",
    "\n",
    "        if content_key in content_keys:\n",
    "            len_response_data = 0\n",
    "            len_assert_data = 0\n",
    "            if response_data and not isinstance(response_data[0], dict):\n",
    "                len_response_data = len(response_data)\n",
    "                for res_str in response_data:\n",
    "                    assert_info.test_account[content_key].add(str(res_str).strip().lower())\n",
    "            if assert_data and not isinstance(assert_data[0], dict):\n",
    "                len_assert_data = len(assert_data)\n",
    "                for res_str in assert_data:\n",
    "                    assert_info.gold_account[content_key].add(str(res_str).strip().lower())\n",
    "            if len_assert_data != len_response_data:\n",
    "                pass\n",
    "                # print(f'values length not equality with content key {content_key} and index {assert_info.val_index}')\n",
    "            \n",
    "        if response_data:\n",
    "            if isinstance(response_data[0], dict):\n",
    "                response_data = sorted(response_data, key=lambda x: x[list(response_data[0].keys())[0]])\n",
    "            else:\n",
    "                response_data = sorted(response_data)\n",
    "        if assert_data:\n",
    "            if isinstance(assert_data[0], dict):\n",
    "                assert_data = sorted(assert_data, key=lambda x: x[list(assert_data[0].keys())[0]])\n",
    "            else:\n",
    "                assert_data = sorted(assert_data)\n",
    "        \n",
    "        \n",
    "        if response_data and assert_data and isinstance(response_data[0], dict):\n",
    "            if content_key in content_keys:\n",
    "                print(f'content key: {content_key} except with dict type object!!!')\n",
    "            for src_list, dst_list in zip(response_data, assert_data):\n",
    "                diff_json(src_list, dst_list, content_keys, assert_info, content_key)\n",
    "    else:\n",
    "        if content_key in content_keys:\n",
    "            if response_data:\n",
    "                assert_info.test_account[content_key].add(str(response_data).strip().lower())\n",
    "\n",
    "            if assert_data:\n",
    "                assert_info.gold_account[content_key].add(str(assert_data).strip().lower())\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "Content_keys = ['Diseases', 'Symptoms', \n",
    "                'DiagnosisMethods', 'Causes', 'MedicalHistory',\n",
    "                'TherapeuticsApproach', 'CureRate', 'DrugFrequency']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#truth results\n",
    "\n",
    "trigger = 'treatment'\n",
    "treatment_vals = []\n",
    "val_results = []\n",
    "for fold in range(5):\n",
    "    val_path = f'./chatbot_checked_{trigger}/fold{fold}/val_{trigger}_fold{fold}.jsonl'\n",
    "    current_val_result = read_jsonl(val_path)\n",
    "    val_results.extend(current_val_result)\n",
    "    for item in current_val_result:\n",
    "        treatment_vals.append(json.loads(item['messages'][2]['content']))\n",
    "\n",
    "trigger = 'cured'\n",
    "cured_vals = []\n",
    "val_results = []\n",
    "for fold in range(5):\n",
    "    val_path = f'./chatbot_checked_{trigger}/fold{fold}/val_{trigger}_fold{fold}.jsonl'\n",
    "    current_val_result = read_jsonl(val_path)\n",
    "    val_results.extend(current_val_result)\n",
    "    for item in current_val_result:\n",
    "        cured_vals.append(json.loads(item['messages'][2]['content']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##openai cured\n",
    "openai_cured_results = []\n",
    "openai_cured_val_index = []\n",
    "run = []\n",
    "idx = -1\n",
    "for fold in range(5):\n",
    "    result_file_name = f\"./chatbot_checked_cured/fold{fold}/batch_results_cured_fold{fold}.jsonl\"\n",
    "    with open(result_file_name, 'r') as file:\n",
    "        for line in file:\n",
    "            # Parsing the JSON string into a dict and appending to the list of results\n",
    "            idx += 1\n",
    "            json_object = json.loads(line.strip())\n",
    "            openai_cured_results.append(json.loads(json_object['response']['body']['choices'][0]['message']['content']))\n",
    "            run.append(json_object['response']['body']['choices'][0]['message']['content'])\n",
    "            openai_cured_val_index.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##openai treatment\n",
    "openai_treatment_results = []\n",
    "openai_treatment_val_index = []\n",
    "run = []\n",
    "idx = -1\n",
    "for fold in range(5):\n",
    "    result_file_name = f\"./chatbot_checked_treatment/fold{fold}/batch_results_treatment_fold{fold}.jsonl\"\n",
    "    with open(result_file_name, 'r') as file:\n",
    "        for line in file:\n",
    "            # Parsing the JSON string into a dict and appending to the list of results\n",
    "            idx += 1\n",
    "            json_object = json.loads(line.strip())\n",
    "            openai_treatment_results.append(json.loads(json_object['response']['body']['choices'][0]['message']['content']))\n",
    "            run.append(json_object['response']['body']['choices'][0]['message']['content'])\n",
    "            openai_treatment_val_index.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Sample\": [{\"Diseases\": \"Secondary hyperaldosteronism\", \"Symptoms\": [], \"TherapeuticsInformation\": [{\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"CureRate\": \"1\", \"DrugFrequency\": \"\"}, {\"TherapeuticsApproach\": \"Atypical antipsychotic\", \"\n"
     ]
    }
   ],
   "source": [
    "##llama cured\n",
    "llama_cured_results = []\n",
    "llama_cured_val_index = []\n",
    "idx = -1\n",
    "cured_run = []\n",
    "for fold in range(5):\n",
    "    current_run = read_jsonl(os.path.join(f'./results_cured_checked_chat', f'run_cured_{fold}.jsonl'))\n",
    "    cured_run.extend(current_run)\n",
    "    for result in current_run:\n",
    "        idx += 1\n",
    "        try:\n",
    "            completion = json.loads(result['completion'])\n",
    "        except:\n",
    "            print(result['completion'])\n",
    "            continue\n",
    "        \n",
    "        llama_cured_val_index.append(idx)\n",
    "        llama_cured_results.append(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Sample\": [{\"Diseases\": \"cancer of the adrenal cortex\", \"Symptoms\": [], \"TherapeuticsInformation\": [{\"TherapeuticsApproach\": \"O,P\"-DDD, \"CureRate\": \"\", \"DrugFrequency\": \"\"}], \"DiagnosisMethods\": [], \"Causes\": \"\", \"MedicalHistory\": \"\"}]}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##llama treatment\n",
    "llama_treatment_results = []\n",
    "llama_treatment_val_index = []\n",
    "idx = -1\n",
    "treatment_run = []\n",
    "for fold in range(5):\n",
    "    current_run = read_jsonl(os.path.join(f'./results_treatment_checked_chat', f'run_treatment_{fold}.jsonl'))\n",
    "    treatment_run.extend(current_run)\n",
    "    for result in current_run:\n",
    "        idx += 1\n",
    "        try:\n",
    "            completion = json.loads(result['completion'])\n",
    "        except:\n",
    "            print(result['completion'])\n",
    "            continue\n",
    "        \n",
    "        llama_treatment_val_index.append(idx)\n",
    "        llama_treatment_results.append(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_match(query, value, sim_model):\n",
    "    device = torch.device(\"cpu\")\n",
    "    embedding_1 = sim_model.encode(query, convert_to_tensor=True, device=device) # encode on CPU\n",
    "    embedding_2 = sim_model.encode(value, convert_to_tensor=True, device=device) # encode on CPU\n",
    "    sim_matrix = util.pytorch_cos_sim(embedding_1, embedding_2)\n",
    "    return sim_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cured\n",
    "openai_cured_assertInfos = []\n",
    "llama_cured_assertInfos = []\n",
    "res_idx = 0\n",
    "openai_cured_gold = []\n",
    "openai_cured_pred = []\n",
    "llama_cured_gold = []\n",
    "llama_cured_pred = []\n",
    "for idx in range(len(cured_vals)):\n",
    "    if idx in openai_cured_val_index:\n",
    "        openai_cured_gold.append(val_results[idx]['messages'][2]['content'])\n",
    "        openai_cured_pred.append(run[idx])\n",
    "        openai_cured_assertInfos.append(AssertInfo(idx, res_idx, Content_keys))\n",
    "        diff_json(openai_cured_results[res_idx], cured_vals[idx], Content_keys, openai_cured_assertInfos[res_idx], 'root')\n",
    "        res_idx += 1\n",
    "        \n",
    "res_idx = 0\n",
    "for idx in range(len(cured_vals)):\n",
    "    if idx in llama_cured_val_index:\n",
    "        llama_cured_gold.append(val_results[idx]['messages'][2]['content'])\n",
    "        llama_cured_pred.append(run[idx])\n",
    "        llama_cured_assertInfos.append(AssertInfo(idx, res_idx, Content_keys))\n",
    "        diff_json(openai_cured_results[res_idx], cured_vals[idx], Content_keys, llama_cured_assertInfos[res_idx], 'root')\n",
    "        res_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#treatment\n",
    "openai_treatment_assertInfos = []\n",
    "llama_treatment_assertInfos = []\n",
    "res_idx = 0\n",
    "openai_treatment_gold = []\n",
    "openai_treatment_pred = []\n",
    "llama_treatment_gold = []\n",
    "llama_treatment_pred = []\n",
    "for idx in range(len(treatment_vals)):\n",
    "    if idx in openai_treatment_val_index:\n",
    "        openai_treatment_gold.append(val_results[idx]['messages'][2]['content'])\n",
    "        openai_treatment_pred.append(run[idx]['completion'].replace('\\\\\\\"',\"\"))\n",
    "        openai_treatment_assertInfos.append(AssertInfo(idx, res_idx, Content_keys))\n",
    "        diff_json(openai_treatment_results[res_idx], treatment_vals[idx], Content_keys, openai_treatment_assertInfos[res_idx], 'root')\n",
    "        res_idx += 1\n",
    "\n",
    "res_idx = 0\n",
    "for idx in range(len(treatment_vals)):\n",
    "    if idx in llama_treatment_val_index:\n",
    "        llama_treatment_gold.append(val_results[idx]['messages'][2]['content'])\n",
    "        llama_treatment_pred.append(run[idx]['completion'].replace('\\\\\\\"',\"\"))\n",
    "        llama_treatment_assertInfos.append(AssertInfo(idx, res_idx, Content_keys))\n",
    "        diff_json(llama_treatment_assertInfos[res_idx], treatment_vals[idx], Content_keys, llama_treatment_assertInfos[res_idx], 'root')\n",
    "        res_idx += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for assertInfos in [openai_cured_assertInfos, openai_treatment_assertInfos, llama_cured_assertInfos, llama_treatment_assertInfos]:\n",
    "    assertlength = len(assertInfos)\n",
    "    structure_negative = 0\n",
    "    for assertItem in assertInfos:\n",
    "        if assertItem.structure_id == False:\n",
    "            structure_negative += 1\n",
    "    structure_postive = assertlength - structure_negative\n",
    "    print(structure_postive / len(cured_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "sim_model = SentenceTransformer('/Users/yuz/Work/reprod/sentence-transformers')\n",
    "sim_model = sim_model.to(device)\n",
    "for gold, pred in [(openai_cured_gold, openai_cured_pred), (openai_treatment_gold, openai_treatment_pred), \n",
    "                   (llama_cured_gold, llama_cured_pred), (llama_treatment_gold, llama_treatment_pred)]:\n",
    "    gold_n, pred_n, total_score = 0, 0, 0\n",
    "    for gold_item, pred_item in zip(gold, pred):\n",
    "        gold_n += len(gold_item)\n",
    "        pred_n += len(pred_item)\n",
    "        scores = soft_match(gold_item, pred_item, sim_model)[0][0]\n",
    "        total_score += scores\n",
    "        # break\n",
    "    print(total_score / len(gold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict = {}\n",
    "for name, assertInfos in [(\"openai_cured\", openai_cured_assertInfos), \n",
    "                    (\"openai_treat\", openai_treatment_assertInfos), \n",
    "                    (\"llama_cured\", llama_cured_assertInfos), \n",
    "                    (\"llama_treat\", llama_treatment_assertInfos)]:\n",
    "    overall_p_scores_rouge = {e: [] for e in Content_keys}\n",
    "    overall_r_scores_rouge = {e: [] for e in Content_keys}\n",
    "    overall_f1_scores_rouge = {e: [] for e in Content_keys}\n",
    "    overall_scores_rouge = {e: [] for e in Content_keys}\n",
    "\n",
    "    overall_p_scores_em = {e: [] for e in Content_keys}\n",
    "    overall_r_scores_em = {e: [] for e in Content_keys}\n",
    "    overall_f1_scores_em = {e: [] for e in Content_keys}\n",
    "    overall_scores_em = {e: [] for e in Content_keys}\n",
    "    for idx in range(len(assertInfos)):\n",
    "        if not assertInfos[idx].structure_id:\n",
    "            continue\n",
    "        for key in Content_keys:\n",
    "            test_account = list(assertInfos[idx].test_account[key])\n",
    "            gold_account = list(assertInfos[idx].gold_account[key])\n",
    "            if (len(test_account) == 0 and len(gold_account) > 0) or (len(test_account) > 0 and len(gold_account) == 0):\n",
    "                overall_p_scores_rouge[key].append(0)\n",
    "                overall_r_scores_rouge[key].append(0)\n",
    "                overall_f1_scores_rouge[key].append(0)\n",
    "                overall_p_scores_em[key].append(0)\n",
    "                overall_r_scores_em[key].append(0)\n",
    "                overall_f1_scores_em[key].append(0)\n",
    "            elif len(test_account) == 0 and len(gold_account) == 0:\n",
    "                overall_p_scores_rouge[key].append(1.0)\n",
    "                overall_r_scores_rouge[key].append(1.0)\n",
    "                overall_f1_scores_rouge[key].append(1.0)\n",
    "                overall_p_scores_em[key].append(1.0)\n",
    "                overall_r_scores_em[key].append(1.0)\n",
    "                overall_f1_scores_em[key].append(1.0)\n",
    "            else:\n",
    "                total_score = 0\n",
    "                em_score = 0\n",
    "                if len(test_account) < len(gold_account):\n",
    "                    for gold_item in gold_account:\n",
    "                        max_item_score = np.float64(0)\n",
    "                        for test_item in test_account:\n",
    "                            if test_item == gold_item:\n",
    "                                em_score += 1\n",
    "                            current_score = metric.compute(predictions = [test_item], references = [gold_item])['rougeLsum']\n",
    "                            if current_score > max_item_score:\n",
    "                                max_item_score = current_score\n",
    "                        total_score += max_item_score\n",
    "                else:\n",
    "                    for test_item in test_account:\n",
    "                        max_item_score = np.float64(0)\n",
    "                        for gold_item in gold_account:\n",
    "                            if gold_item == test_item:\n",
    "                                em_score += 1\n",
    "                            current_score = metric.compute(predictions = [test_item], references = [gold_item])['rougeLsum']\n",
    "                            if current_score > max_item_score:\n",
    "                                max_item_score = current_score\n",
    "                        total_score += max_item_score\n",
    "                # print(f'key :{key} score: {total_score} len: {len(test_account)}')\n",
    "                p_em_score = em_score / len(test_account)\n",
    "                r_em_score = em_score / len(gold_account)\n",
    "                overall_p_scores_em[key].append(p_em_score)\n",
    "                overall_r_scores_em[key].append(r_em_score)\n",
    "                if p_em_score == 0 and r_em_score == 0:\n",
    "                    overall_f1_scores_em[key].append(0)\n",
    "                else:\n",
    "                    f1 = 2 * ( p_em_score * r_em_score) / (p_em_score + r_em_score)\n",
    "                    overall_f1_scores_em[key].append(f1)\n",
    "                p_score = total_score / len(test_account)\n",
    "                r_score = total_score / len(gold_account)\n",
    "                overall_p_scores_rouge[key].append(p_score)\n",
    "                overall_r_scores_rouge[key].append(r_score)\n",
    "                if p_score == 0 and r_score == 0:\n",
    "                    overall_f1_scores_rouge[key].append(0)\n",
    "                else:\n",
    "                    f1 = 2 * ( p_score * r_score) / (p_score + r_score)\n",
    "                    overall_f1_scores_rouge[key].append(f1)\n",
    "                    \n",
    "    scores_dict[name] = {}\n",
    "    scores_dict[name][\"overall_p_scores_rouge\"] = overall_p_scores_rouge\n",
    "    scores_dict[name][\"overall_r_scores_rouge\"] = overall_r_scores_rouge\n",
    "    scores_dict[name][\"overall_f1_scores_rouge\"] = overall_f1_scores_rouge\n",
    "    scores_dict[name][\"overall_scores_rouge\"] = overall_scores_rouge\n",
    "    \n",
    "    scores_dict[name][\"overall_p_scores_em\"] = overall_p_scores_em\n",
    "    scores_dict[name][\"overall_r_scores_em\"] = overall_r_scores_em\n",
    "    scores_dict[name][\"overall_f1_scores_em\"] = overall_f1_scores_em\n",
    "    scores_dict[name][\"overall_scores_em\"] = overall_scores_em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in [\"openai_cured\", \"openai_treat\", \"llama_cured\", \"llama_treat\"]:\n",
    "    avg_p_scores_em, avg_r_scores_em, avg_f1_scores_em, avg_p_scores_rl, avg_r_scores_rl, avg_f1_scores_rl = 0, 0, 0, 0, 0, 0\n",
    "    for key in Content_keys:\n",
    "        avg_p_scores_em += np.average(scores_dict[name][\"overall_p_scores_em\"][key])\n",
    "        avg_r_scores_em += np.average(scores_dict[name][\"overall_r_scores_em\"][key])\n",
    "        avg_f1_scores_em += np.average(scores_dict[name][\"overall_f1_scores_em\"][key])\n",
    "        avg_p_scores_rl += np.average(scores_dict[name][\"overall_p_scores_rouge\"][key])\n",
    "        avg_r_scores_rl += np.average(scores_dict[name][\"overall_r_scores_rouge\"][key])\n",
    "        avg_f1_scores_rl += np.average(scores_dict[name][\"overall_f1_scores_rouge\"][key])\n",
    "    print(f'{name} avg_p_scores_em: {avg_p_scores_em / len(Content_keys)}')\n",
    "    print(f'{name} avg_r_scores_em: {avg_r_scores_em / len(Content_keys)}')\n",
    "    print(f'{name} avg_f1_scores_em: {avg_f1_scores_em / len(Content_keys)}')\n",
    "    print(f'{name} avg_p_scores_rouge: {avg_p_scores_rl / len(Content_keys)}')\n",
    "    print(f'{name} avg_r_scores_rouge: {avg_r_scores_rl / len(Content_keys)}')\n",
    "    print(f'{name} avg_f1_scores_rouge: {avg_f1_scores_rl / len(Content_keys)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
